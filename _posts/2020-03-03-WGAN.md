---
layout: post
title:  "Wasserstein GAN"
subtitle:   "Paper Review"
categories: Artificial_Intelligence
comments: true
use_math: true 

---

### Wasserstein GAN

>  이 게시글은  ` Martin Arjovsky ` 가 작성한 [Wasserstein GAN](https://arxiv.org/pdf/1701.07875.pdf) 논문을 리뷰한 것으로 잘못된 부분이 있을 수 있음을 알리는 바입니다.

  이번주 리뷰할 논문은 Wasserstein GAN으로 GAN의 단점을 보완한 모델입니다. 논문 제목에서 보면 Wasserstein 이라는 저한테는 굉장히 생소한 단어로 다가와 검색을 해본 결과 Wasserstein distance 라는 거리를 측정하는 식을 이용하였는데 어마무시한 녀석입니다. 수학적 베이스는 [Wasserstein GAN 수학 이해하기](https://www.slideshare.net/ssuser7e10e4/wasserstein-gan-i) 를 참고 하였고 논문을 읽기 전 [GAN — Wasserstein GAN & WGAN-GP](https://medium.com/@jonathan_hui/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490) 을 읽어 조금 더 논문을 쉽게 이해할 수 있었습니다.



들어가기 전에 용어를 한번 정리 하자면

| Term                                                         | Explanation                                                  |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [manifold(다양체)]([https://ko.wikipedia.org/wiki/%EB%8B%A4%EC%96%91%EC%B2%B4](https://ko.wikipedia.org/wiki/다양체)) | 각 포인트 근처의 유클리드 공간과 지역적으로 유사한 토폴리지 공간 이 유클리드 공간의 차원이 n 인 경우, n-매니폴드 |
| [support(지지집합)]([https://ko.wikipedia.org/wiki/%EC%A7%80%EC%A7%80%EC%A7%91%ED%95%A9](https://ko.wikipedia.org/wiki/지지집합)) | 실수형 함수 f는 0으로 매핑되지 않은 요소들을 포함하는 도메인의 하위 집합 |

![스위스롤](/assets/img/swis.png) 매니 폴드의 대표적인 예시 : 스위스롤

##### 1. GAN의 문제점

 GAN이 실제 이미지 생성에서 좋은 성능을 보이고 있는 것은 사실입니다. 하지만 학습 속도가 느리고 학습 과정에서 많은 문제점이 발견되었습니다.

  GAN의 문제점은`Discriminator`와 `Generator`간의 균형을 유지하며 학습하기가 어렵고, 학습이 완료된 이후에도 `Model dropping`이 발생하는 문제가 존재한다는 것입니다. 이러한 문제는 `Discriminator`에서 발생하는데 Discriminator의 학습 과정에서 문제가 생기는 것을 `KL divergency`가 실제 데이터 분포와 모형 데이터 분포 사이에 타당한 접점이 존재하지 않는다면 정의되지 않거나 무한대로 튀는 부분에서 학습이 실패한다는 것을 알 수 있습니다. 



  자세히 들여다 보면 KL Divergency(이하 측도)의 최소화가 성립되려면 생성모형의 밀도가 존재해야합니다. 하지만 실제 세상 데이터의 차원은 인위적으로 매우 높은 차원으로 표현 하지만 결국 저차원 매니폴드에서 매우 밀집된 형태로 나타냅니다.결국 실제 세상에 데이터 같은 경우 일반적으로 차원에 비해 확률 분포가 매우 낮은 차원의 매니폴드로 밀집되어있고 이럴경우 확률 분포가 밀집한 `매니폴드의 측도는 0`이 됩니다. (즉 거의 동일하다.)

 그럼  [절대 연속 측도 이론]([https://ko.wikipedia.org/wiki/%EC%A0%88%EB%8C%80_%EC%97%B0%EC%86%8D_%EC%B8%A1%EB%8F%84](https://ko.wikipedia.org/wiki/절대_연속_측도))에 의해서 원래 측도의 값이 0 일 경우 절대 연속 측도의 값이 0이어야 한다.  결국 이러한 경우 매니폴드 $$M$$ 에 대해서 $$P(X \in M) = 0$$이라는 값이 나오고 M을 고려하였을 때  이상한 결과이기에 KL Divergency의 최소화가 적절하지 않다는 것을 알 수 있습니다.   

Wasserstein GAN은 이러한 문제점을 해결하기 위해 아래와 같은 점을 추가 또는 변경하였습니다.

* Discriminator 대신 새로 Critic을 정의하여 사용합니다.
* critic은 Wasserstein distance([EM distance]([https://en.wikipedia.org/wiki/Earth_mover%27s_distance](https://en.wikipedia.org/wiki/Earth_mover's_distance)))로 부터 얻은 scalar 값을 이용합니다.
* [Jensen Shannon Divergence]([https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence](https://en.wikipedia.org/wiki/Jensen–Shannon_divergence))를 사용하여 minmax 문제를 해결하던 GAN과 달리  결합 분포들을 구하여 max 문제를 해결하도록 하였습니다.

  결국 가장 큰 차이점은 GAN의 비용함수를 Wasserstein Distance로 설정하여 최적화를 진행하고 Discriminator 대신 `Critic`을 사용하였다는 것이 가장 큰 차이점이다.

##### 2. EM의 타당성

EM 거리의 타당성을 논하기전 대표적인 거리를 몇가지 살펴보자.

* Total Variation distance


$$ \delta (P_r,P_g)$$ $$ = \sup_{A \in \sum} $$ $$ \vert P_r(A)−P_g(A) \vert $$

두 확률 분포의 측정값이 벌어질 수 있는 가장 큰 값을 뜻합니다.

  

* Kullback-Leibler divergence

  $$ KL(P_r \parallel P_g)=\int P_r(x)logP_r(x)P_g(x)dx $$ 

  두 확률 분포가 얼마나 다른지를 측정한 값을 뜻합니다.

  

* Jensen-Shanon divergence

  $$ JS(P_r,P_g)=KL(P_r \parallel P_m)+KL(P_g \parallel P_m),P_m={(P_r+P_g)\over 2} $$

  KL 두가지를 구하고 평균을 내는 방식 입니다.

  

* Earth-Mover distance, Wasserstein-1 distance

$$ W(P_r,P_g)= \inf_{\gamma {\in} {\mathsf{\Pi}} (P_r,P_g)}E_{(x,y)∼\gamma}( \parallel x−y \parallel ) $$

 두 확률 분포의 결합확률 분포 $$  \mathsf{\Pi} (P_r,P_g) $$ 중 x 와 y의 거리의 기대값을 가장 작게 추정한 값을 뜻합니다.



논문에서는 각 거리 함수와 EM distance를 비교하며  Example 1을 통해 EM distance의 타당성을 이야기합니다.

![Example 1](/assets/img/WGAN_Example1.png)[출처] Wasserstein GAN

  기존 GAN에서 `Discriminator`와 `Generator`가 확률 분포를 학습할 때 EM 거리를 제외한 나머지 거리를 사용한다면 $$\vert\theta\vert$$ 가 일정한 상수 또는 무한대를 유지하다가 갑작스럽게 0 으로 변함으로 기울기가 잘 전달 안되고 이러한 경우 학습에 문제가 있다는 것을 보여줍니다.  EM distance의 경우 분포가 겹치던 겹치치 않던 간에 $$\vert\theta\vert$$를 유지함으로, 학습하기에 조금 더 쉽다는 것을 알 수 있습니다.

논문에서는 이러한 예시를 들면서 다른 거리에서는 수렴하지 않는 확률 분포의 수열이 EM 거리에서는 수렴 하는 경우가 있다는 것을 알려줍니다. 시각적으로 표현 하였을 때 아래의 그림과 같습니다.

![Example 2](/assets/img/WGAN_Example2.png)[출처] Wasserstein GAN



##### EM distance를 사용해도 괜찮을까?

이제 우리는 EM 거리가 학습할 때 KL 보다 조금 더 유용하다는 것을 알았습니다. 하지만 이걸 그대로 사용할 수 있을지는 아직 미지수 입니다. 논문은 `Theorem 1`과 `Theorem 2`를 통해 연속성과 미분가능성, 분포 수렴은 동치 관계라는 것을 증명하였습니다. 그리고 마지막으로 `Backpropagation`을 하기 위해 미분 값이 필요함으로 미분 값에 대해 `Theorem 3`으로 정의 하였습니다.

##### Theorem 1

$$P_r$$ 을 에 $$\chi$$ 대한 고정 분포라고 하고, $$Z$$를 다른 공간 $$Z$$에 대한 임의의 변수라고 하자. 그리고 $$g:Z \times \mathbb{R}^d \rightarrow  \chi $$ 라고 하자. $$Z$$ 는 첫번째 좌표이고 $$\theta$$ 는 두번째 좌표인 $$g_\theta(Z) $$로 표시된다. $$P_\theta$$ 가 $$g_\theta(Z)$$의 분포를 나타내도록 할 때 ,

1. g가 $$\theta$$ 에서 연속이면 $$W(P_r,P_\theta) $$도 연속입니다.
2. g가 Lipshitz조건과 1번을 만족하면 $$W(P_r,P_\theta) $$는 어디에서나 연속적이며 거의 모든 곳에서 미분 가능합니다.
3. Jensen-Shannon divergence $$JS(P_r,P_\theta)$$ 및 $$KL(P_r,P_\theta)$$는 1,2번에 해당하지 않습니다.

  여기서 $$P_r$$ 은 최종 학습분포 이며 $$P_\theta $$는 학습 시키고 있는 현재의 분포라고 생각 할 수 있다. $$Z$$는 latent Variable이며, 함수 g는 $$Z$$를 $$\chi$$로 사상하는 함수이다. $$g_\theta(Z)$$의 분포는 $$P_\theta$$가 된다.



여기서 [**Lipshitz**]([https://ko.wikipedia.org/wiki/%EB%A6%BD%EC%8B%9C%EC%B8%A0_%EC%97%B0%EC%86%8D_%ED%95%A8%EC%88%98](https://ko.wikipedia.org/wiki/립시츠_연속_함수))조건이란 두 점 사이의 거리를 일정 비 이상으로 증가시키지 않는 함수이다. 수식으로 표현 하자면 $$ \vert f(x_1) - f(x_2)\vert \le K\vert x_1 - x_2\vert$$ 이렇게 간단하게 표시할 수 있다. 그럼 이제 립시츠 조건을 어떻게 만족시킬지가 관건인데 이건 **Theorem 3** 에 나온다.



##### Theorem 2

$$\mathbb{P}$$ 를 컴팩트한 공간 $$\chi $$  에서의 확률분포라고 하고 $$(\mathbb{P}_n)_{n \in \mathbb{N}}$$를 $$\chi$$ 에 대한 일련의 확률 분포라고 하자. $$n \rightarrow \infty$$로 극한을 고려할 때 ,

1. 아래 내용은 모두 같습니다.
   * $$ \delta(\mathbb{P}_n, \mathbb{P}) \rightarrow 0 $$ 여기서 δ는 Total variation distance입니다.
   * $$JS(\mathbb{P}_n,\mathbb{P}) \rightarrow 0$$ 여기서 JS는 Jensen-Shanon divergence 입니다.
2. 아래 내용은 모두 같습니다.
   * $$W(\mathbb{P}_n,\mathbb{P}) \rightarrow 0$$
   * $$\mathbb{P}_n  \overset{D}{\rightarrow} \mathbb{P}$$  여기서  $$\overset{D}{\rightarrow} $$  는 랜덤 변수 분포의 수렴을 나타냅니다.
3. $$KL(\mathbb{P}_n\vert\vert\mathbb{P}) \rightarrow 0$$ or $$KL(\mathbb{P}\vert\vert\mathbb{P}_n) \rightarrow 0$$ 는 1번을 의미합니다.
4. 1번은 2번을 의미합니다.

Theorem 2** 같은 경우  EM거리와 분포 수렴이 동치 관계라는 것을 알려주고 있습니다. 결국 동치관계에 4가지 거리 중에서 EM거리가 조금 더 학습에 도움이 된다는 것을 강조하고 있습니다.



#### 3. Wasserstein GAN

이제 `Wasserstein-1 distence`를 이용한 비용 함수로 GAN의 Loss Function을 논문에서는 정의 하였습니다.
$$
W(P_r,P_g)=\inf_{\gamma \in  \mathsf{\Pi} (P_r,P_g) }E_{(x,y) \sim \gamma}[\parallel x−y\parallel ]
$$
기본적으로 EM 거리는 $$\underset{\gamma \in \mathsf{\Pi} (P_r,P_g)}{inf}$$ 부분을 계산하여야 하지만 실제로는 계산을 하지 못한다. 이유는 $$P_r$$ 과 $$P_g$$의 결합분포를 계산해야하는데 $$P_r$$이 우리의 최종 목표 분포임으로 정답을 알고있어야만 학습을 시킬 수 있다는 결론이 나옵니다. 이에 논문의 필자는 [ Kantorovich-Rubinstein duality](https://eudml.org/doc/268473#content)를 이용하여 새롭게 변형된 형태를 제안하였습니다. 제안한 식은 아래와 같습니다. 자세한 변환식을 보고싶다면 [Wasserstein GAN and the Kantorovich-Rubinstein Duality](https://vincentherrmann.github.io/blog/wasserstein/)이 포스트를 참고하면 좋을 것 같다.
$$
W(\mathbb{P}_r,\mathbb{P}_\theta) = \underset{\parallel f\parallel_L\le 1}{\sup}\mathbb{E}_{x \sim \mathbb{P}_r}[f(x)] - \mathbb{E}_{x \sim \mathbb{P}_\theta}[f(x)]
$$
$$\sup$$ 는 least upper bound를 측정하고자 하는 것 즉 최대값을 의미합니다.  여기서 $$\parallel f \parallel_L \le 1$$ 의 의미는 f가 1-Lipschitz 조건을 만족한다는 뜻이다. 결국 K-Lipshitz continuous 조건을 만족해야 한다. 여기서 K는 $$f$$의 립시츠 상수라고 부릅니다. 모든 점에서 연속적으로 미분 가능한 함수는 립시츠 연속 함수여야 합니다. 미분이 가능하다는 것은 $$ {\vert f(x_1) - f(x_2)\vert\over{\vert x_1 - x_2\vert}} \le K$$  미분 값이 항상 K보단 작다라고 제한되어 있음을 의미하기 때문이다.하지만 반대로 립시츠 연속함수라고 해서 항상 모든점에서 미분 가능하지는 않습니다. 자세한건  [**립시츠연속함수**]([https://ko.wikipedia.org/wiki/%EB%A6%BD%EC%8B%9C%EC%B8%A0_%EC%97%B0%EC%86%8D_%ED%95%A8%EC%88%98](https://ko.wikipedia.org/wiki/립시츠_연속_함수)) 위키백과를 통해 알 수 있다. 결국 위의 변형식 또한 미분 가능하다는 것을 알 수 있다. 그리하여 위의 식에서 매개변수 추가된 $$f_w$$와 $$g_\theta(z)$$로 식을 바꾸면 
$$
W(\mathbb{P}_r,\mathbb{P}_\theta) = \max_{w\in W}\mathbb{E}_{x \sim \mathbb{P}_r}[f_w(x)] - \mathbb{E}_{z \sim p(z)}(f_w(g_\theta(Z)))
$$
로 표현 가능하고 이 때 $$f(x)$$는 립시츠 조건을 만족하는 함수로, Critic 역할을 하는 함수 입니다. 위 식은 GAN의 손실 함수와 비슷한 모습임을 볼 수 있다. 이제 학습을 진행할려면 미분식이 필요합니다. 이것을 논문에서는 **Theorem 3**에서 알려줍니다.


##### Theorem 3

$$\mathbb{P}_r$$ 을 어떤 분포라고 하고 $$\mathbb{P}_\theta $$ 는 밀도 p를 갖는 랜덤 변수 Z에 대한 $$g_\theta(Z)$$의 분포이다. 이 때 $$f: \chi \rightarrow \mathbb{R}$$이 $$\max_{w\in W}\mathbb{E}_{x \sim \mathbb{P}_r}[f_w(x)] - \mathbb{E}_{z \sim p(z)}[f_w(g_\theta(Z))]$$에 대해 해가 존재하며, 그리고 우리는 두 항이 명확하게 정의되었을 때 도함수 $$ \nabla _ \theta W(P_r, P_ \theta) = −E_{z \sim p(z)} [ \nabla _ \theta f(g_ \theta(z))]$$ 를 갖는다. 



이렇게 미분식 또한 존재 하여 BackPropagation할 때 사용할 수 있는 것을 알 수 있습니다. 간단히 $$\theta $$에 대한 편미분을 통해 첫번째 항이 소거 되고 두번째 항만 남았다는 것을 알 수 잇습니다. 그리하여 위의 식에 근거하여 모수 $$\theta$$를 학습하게 됩니다.

  

  결론적으로 WGAN에선 진짜와 가짜를 식별하는 Discriminator는 더이상 존재하지 않습니다. 대신 EM거리를 계산 하기 위해 사용되는 립시츠 연속함수를 학습하게 됩니다. 학습과정에서 손실함수가 작아질 수록 EM 거리는 작아지고 Generator의 결과 값은 실제와 비슷해집니다. 하지만  한가지 문제점이 존재하는데 학습 과정에서 K-립시츠 연속을 유지 해야 하는 것 입니다. 논문에서는 기울기가 업데이트될 때마다, 가중치 w를 아주 작은 범위로 , 논문에서는 [-0.01,0.01]로 고정 시켜 컴팩트한 파라미터 공간 W가 되도록 합니다. 그럼 $$f_w $$는 하한과 상한이 생기게 되며 립시츠 연속을 유지하게 됩니다.

##### 알고리즘

아래는 논문에서 제시한 WGAN 알고리즘입니다.

![algorithm](/assets/img/WGAN_algorithm.png)[출처] Wasserstein GAN

$$n_{critic}$$: critics을 돌릴 횟수 논문에서는 5회 반복합니다.

Critics

1. $$\mathbb{P}_r$$로 부터 m개의 $$x $$들을 샘플링 합니다. (m은 batch)
2. $$\mathbb{P}_g$$로 부터 m개의 $$z $$들을 샘플링 합니다.
3. 가중치에 대한 미분값 $$g_w \leftarrow  \nabla _w[{1\over m} \sum_{i=1}^m f_w(x^{(i)}) - {1\over m} \sum_{i=1}^m f_w(g_ \theta(z^{(i)}))$$를 계산합니다.
4. 가중치 업데이트 : $$w \leftarrow w + \alpha RMSProp (w,g_w)$$

RMSProp : 여기서 Adam을 안 쓴 이유는 Adam을 사용할 경우  Adam이 이전에 기억했던 방향과 기울기의 방향 간의 $$\cos$$ 값이 음수가 됩니다.

5. 가중치 제약(weight clipping): $$w \leftarrow clip(w,-c,c)$$ [논문에서는 c=0.05로 주었다.]

   가중치를 clipping을 해주어 어떠한 특정 값들을 넘어가지 않도록 해주었습니다.



Generator

1. $$\mathbb{P}_g $$로 부터 m개의 $$z$$들을 샘플링 합니다.
2. Generator에 대한 미분값 :  $$ g_\theta \leftarrow $$ $$ \nabla _ \theta {1\over m}[-\sum_{i=1}^m f_w(z^{(i)})$$
3. Generator 업데이트 : $$\theta \leftarrow \theta - \alpha RMSProp (\theta,g_\theta) $$

시각적으로 표현하자면 아래와 같다.

![archi](/assets/img/WGAN_archi.png)

  WGAN은 GAN보다 학습속도나 안정성에 관하여 더 발전된 모델이지만 논문에서는 WGAN의 문제점을 언급합니다.바로 clipping 해주는 과정에서 문제가 생기는데  본래 clipping 해주는 이유는 파라미터 W가 립시츠 조건을 만족하도록 하기 위해서 강제로 clipping 해주는 것입니다. 하지만 c가 크면 극한 까지 도달 하는 시간이 오래 걸리고, 최적 지점까지 학습하는 데 시간이 오래 걸립니다. 반면 c가 작을 경우, 기울기 사라짐 문제가 발생한다고 논문에서는 언급합니다. 결국 성능이 잘 뽑히고 간결하여 사용하였지만 이 후 더 발전된 모델에선 립시츠 조건을 만족시키는 다른 방법을 다른 학자들에게 맡긴다고 논문에서는 말합니다.



나머지 부분은 이해하는 데 크게 어려움이 없을 꺼라 생각합니다.
