layout: post
title:  "강화학습 1 :: Introduction to Reinforcement Learning "
subtitle:   "lecture review"
categories: Artificial_Intelligence
comments: true



#### 강화학습 1 :: Introduction to Reinforcement Learning

> David Silver의 Reinforcement Learning [강의자료](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html) [유튜브](https://www.youtube.com/watch?v=2pWv7GOvuf0&t=24s)  를 참고하여 작성한 포스팅으로
>
> 잘못된 부분이 있을 수도 있음을 알립니다. 피드백 감사히 받겠습니다.

포스팅의 순서는 강의자료 순서와 똑같은 흐름대로 

1. 강화학습이란? 
2. 강화학습 문제 
3. 강화학습 에이전트의 구성요소 
4. 강화학습의 문제

이 순서대로 진행할 것 같습니다.



###### 1. 강화학습이란?

기계 학습에는 지도 학습 , 비지도 학습 , 강화학습  세가지로 분류 됩니다. 그 중 강화 학습은 현재 상태(State)에서 어떤 행동(Action)을 취하는 것이 최적인지 아닌지 학습 하는 것입니다. 강화학습의 특징은 총 4가지로 아래와 같이 분류 됩니다. 

> 1.  There is no supervisor, only a reward signal
>
>    강화 학습에는 supervisor, 즉 정답을 알려주지 않고 reward(보상)신호만을 통해서 학습을 진행합니다.
>
>    정답을 알려주는 행위와 보상을 주는 행위가 유사하게 느껴진다. 하지만 정답을 알려주는 것은 " 넌 무조건 이렇게 해야만 해!" 라면 보상을 주는 행위는 "너가 알아서 보상을 최대로 많이 받아봐봐" 라며 행위에 따른 보상만 주어질 뿐 어떠한 해결책을 주진 않는다.  
>
> 2. Feedback is delayed, not instantaneous
>
>    강화학습의 경우 Feedback(보상)이 즉시 주어지지 않을 수 있다. 이 때문에 문제의 난이도가 다른 학습에 비해 대폭 상승하며, 시스템을 제대로 보상하는 것과 관련된 신뢰 할당 문제라는 난제가 생겼습니다.
>
> 3. Time really matters (sequential, non i.i.d data)
>
>    강화학습은 i.i.d(Independent and Identically distribution) data가 아닌 Sequential  데이터로 순서가 존재함으로 순서와 시간이 중요하다고 한다.
>
> 4. Agent's actions affect the subsequent data it receives
>
>    Agent의 동작이 그 이후 받게 될 데이터에 대해 영향을 준다. 강화학습은 어떻게 피팅 하는가에 따라 순간적으로 입력 데이터가 달라진다.





###### 2. 용어 정리 

1. Rewards

   1. 보상 $$R_t$$  는 t-step에 Action에 주어지는 스칼라로 하나의 숫자 신호이다.
   2. Agent가 t 단계에서 얼마나 잘하고 있는 지를 나타내는 지표
   3. Agent의 목적은 누적된 $$R_t$$를 극대화 하는 것

   > Definition of Reward hypothesis
   >
   > All goals can be described by the maximisation of expected cumulative reward

   reward hypohesis란? 

   학습 중에 모든 행동(Action)은 누적된 $$R_t$$를 최대화 하기 위한 의도를 가지고 일어난 행동(Action) 입니다. 

     강화학습은 이 reward hypothesis에 기초합니다. 왜 그러면 누적된 $$R_t$$로 계산을 할까라고 생각을 한다면, 월터 미셸의 마시멜로 실험를 떠올릴 수 있습니다. 마시멜로 실험은 1개의 마시멜로를 주고 15분을 기다릴 경우 1개의 마시멜로를 더 주는 실험으로 , 눈 앞의 보상은 마시멜로 1개지만, 기다린다면 보상 2를 획득 할 수 있는 실험이다. 누적된 보상이 아닌 그 때 그 때 보상이 주어진다면 강화학습은 Greedy 한 알고리즘으로 변할 것이지만 누적된 보상을 줌으로써 전체를 바라보는 알고리즘으로 학습됩니다 .

     결국 강화학습은 잘하고 못하고가 명확해야하며 실생활의 문제를 강화학습 문제로 치환을 할려면 주어지는 reward가 복잡한 벡터가 아닌 스칼라로 치환 해야한다는 것을 알 수 있습니다. 

2. Agent and Environment

![RL3](/Users/Yoon/workspace/yoonyoung97.github.io/assets/img/RL3.png) 

위 그림은 Agent 와 Environment를 이해하기 쉽게 설명하기 위해 뇌와 지구로 비유를 든 그림입니다.

Agent 는 상태를 관찰, 행동을 선택, 보상을 최대화 하는 목표를 가졌으며, 여기서는 뇌로 비유 되었습니다.

Environment는 에이전트를 제외한 나머지로 여기서는 지구로 비유되었습니다.

여기 그림을 토대로 설명을 하자면 , Agent가  Action $$A_t$$를 할 경우 Environment가 $$A_t$$를 받아서 Agent에게 너가 한 $$A_t$$ 때문에 상황이 바뀌었다며 observation $$O_{t+1}$$ 와 $$A_t$$에 대한 rewards $$R_{t+1}$$을 보내는 식의 과정을 반복합니다. 

3. History and State

   History

     History는 말그대로 기록 입니다. Agent가 작동한 것을 기록으로 남겨놓습니다.

   $$H_t = O_1,R_1,A_1,...,A_{t-1},O_t,R_t $$ 

   그래서 $$H_t$$는 Time $$t$$ 까지 기록을 의미합니다.  여기서 $$O$$는 Observation, $$R$$은 rewards, $$A$$는 Action 입니다.  그래서 Agent는 History를 보고 Action을 정하고 Environmnet는 History를 보고 Agent에게 Observation 과 rewards를 보내줍니다.

   State

     Agent와 Environment가 다음 행동을 결정하기 위해 쓰이는 데이터입니다. 

   $$S(t) = f(H_t)$$라는 수식으로 이루어진 State는 결국 History로 이루어진 함수입니다.

   

   Agent의 state는 다음의 Action을 위해 쓰이는  숫자 데이터

   

   

   Environment의 state는 다음 Agent에게 Observation 과 rewards를 주기 위해 쓰이는 숫자 데이터 입니다.

   

   

   

