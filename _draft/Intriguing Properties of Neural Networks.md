# Intriguing properties of neural networks

### **Abstract**

  딥 뉴럴 네트워크는 음성 및 시각적 인식 작업에서 성능이 엄청나게 뛰어난 모델이다. 그들의 뛰어난 성능은 성공의 이유가 되었지만, 내부의 작동방식을 이해할 수 없기 때문에 해석할 수 없는 솔루션을 배우게 됩니다. 이러한 딥뉴럴 네트워크의 특성은 두가지 정도 있습니다.

첫번째,  unit의 분석 방법에 따라 individual high level unit 과 random linear combinations의 high level unit과 차이가 없다고 합니다.

여기서 유닛은 각 레이어층 을 의미한다고 볼 수 있다. LSTM 유닛 부터 Dense Unit 등등

둘째, 심층 신경망은 상당히 불연속적인 결과를 학습합니다.네트워크 예측 오류를 최대화하여 네트워크가 이미지를 잘못 분류하도록 유도할 수 있습니다.

#### Introdution

딥뉴럴네트워크에서 신경망은 선형적인 계산 뿐만 아니라 레이어를 여러개 섞어 비선형 또한 계산할 수 있어 높은 성능을 뽐낼 수 있다. 결과 계산은 지도 학습을 통한 역전파에 의해 자동으로 발견되기 때문에 인간이 해석하기에는 어렵고 직관적이지 않은 특성을 가지고 있다.

본 논문에서는 심층 신겸앙의 두가지 반 직관적 특성에 대해 논의한다

**First property** 

  첫번째 특성은 각각의 유닛의 의미론적 의미와 관련이 있다. 이전 논문[6,13,7]은 주어진 유닛을 최대한 활성화 시키는 입력 세트를 찾아서 다양한 유닛의 의미론적 의미를 분석했다. 여기서 의미론적 의미(Semantic meaning은 컴퓨터가 해석하는 의미? 느낌으로 해석을 하면 편하다). 각각의 유닛의 검사에서 최종 특징 층의 유닛이 의미적 정보를 추출하는데 유용하게 구별할수 있는 정보를 형성한다는 암시적인 가정을 하다.

우리는 섹션 3에서 $$\sigma(x)$$의 랜덤 투영값이 의미적으로 $$\sigma(x)$$ 의 좌표와 구별 할 수 없음을 보여줍니다. 이것은 신경망이 좌표에 기반하여 분류값들의 기준을 세워 분리 시킨다는 추측에 의문을 들게 해줍니다. 일반적으로 대량의 데이터를 포함하는 것은 각각의 유닛이 아닌 전체 유닛으로 이뤄진 네트워크 인것으로 보인다.

Mikolov의 word2Vec에서 단어 표현에 있어서, 단어를 나타내는 벡터 공간의 다양한 방향이  풍부한 관계 및 유사성의 의미론적 인코딩을 야기하는것으로 도시된다. 거나 , 이게 무슨 말이냐면 Word2Vec에서 다차원 공간에서 단어를 벡터화함으로써 단어 사이의 유사도를 측정할수도, 단어와 단어사이의 연산 또한 가능하게 해준다. [벡터 표현은 공간의 회전까지 안정적이므로 벡터 표현의 ㄱ별 단위는 의미 정보를 포함하지 않을 수 있습니다.]

그림 첨부해야함

#### Second Property

  입력값에 대한 미세한 변동과 관련하여 신경망의 안정성이 위협됩니다. 잘 최적화 된 객체 인식 모델을 생각해보자. 작은 노이즈에 이미지 인식 범주를 변경할 수 없기 때문에 , 네트워크는 이러한 작은 노이즈에 강하다. 하지만  인식 못하는 작은 노이즈를 테스트 이미지로 주면 네트워크의 예측을 변경할 수 있다. 우리는 이렇게 만들어진 예를 adversarial example 이라고한다. 논문에서 이러한 adversarial example은 통계치에 크게 영향을 미치지 않고, 학습이 잘된다고한다.  즉 뉴럴로 만든 adversarial example은 다른 네트워크에서 rubost하다는걸 알 수 있다. 이러한 결과는 backpropagation으로 학습된 뉴럴 네트워크가 우리가 알 수 없는 비직관적인 특성과 사각지대를 가지며, 불정확한 방식으로 데이터 분포를 정한다는 것을 알 수있다.

![image-20200512153944123](C:\Users\USER\AppData\Roaming\Typora\typora-user-images\image-20200512153944123.png)



### 논문에서의 환경

​		$$x \in R^m$$  :: input Image

​		$$\phi(x)$$ : Layer 의 activation value

​		논문에서는  이미지의 $$\phi(x)$$를 검사 한 다음 사각지점을 검색할 것이다.

​		MNIST 데이터셋

​		하나 이상의 히든 레이어와 소프트 맥스가 포함된  FCNet을 이용함 

​		오토 인코더 사용

​		데이터셋을 람다 정규화를 이용함

![image-20200512160913398](C:\Users\USER\AppData\Roaming\Typora\typora-user-images\image-20200512160913398.png)

​		여기서 정규화 함수는 

​		![image-20200512160450113](C:\Users\USER\AppData\Roaming\Typora\typora-user-images\image-20200512160450113.png)

​																				[람다 정규화 이전]

​					![image-20200512160504440](C:\Users\USER\AppData\Roaming\Typora\typora-user-images\image-20200512160504440.png)

​																		     [람다 정규화 이후]

​		ImageNet 데이터셋 

​		AlexNet 사용

​		유튜브에서 뽑아넨 10M 이미지 샘플

​		QuocNet을 사용함

### Unit of $$\phi(x)$$ 

​		전통적인 컴퓨터 비전 시스템에서는 피쳐 추출에 힘이 쏠렸다. 이렇게 발전한 연구들은 공간의 좌표들을 검색하고, 입력 이미지들을 변형하고 연결시킬 수 있다.  

육안으로 검사 가능한 이미지 x`

히든 레이어와 관련된 기저벡터 : $$e_i$$

임의의 방향에 해당하는 벡터 : v

​	$$x` = \arg\max_{x\in L}(\phi(x),e_i)$$

​	$$x` = \arg\max_{x\in L}(\phi(x),v)$$



이러한 식은 결국 눈으로 구분하는 것 보다 $$\phi(x)$$ 가 뒤떨어진다는 것을 알 수 있다. 이것은 DNN이 좌표에 걸쳐서 인식한다는 개념에 의문을 제기한다.  

  먼저 우리는 MNIST에 대해 훈련된 컨볼루션 신경망을 사용하여 위의 주장을 평가하였다. 

!그림 1-2

그림 1 히든 레이어의 액티베이션 값으로 최대화 하는 이미지

특정 합성곱 층의 활성화 값을 최대화 하기 위해서 입력

그림 2 임의의 방향으로 활성화를 최대화 하는 이미지 

결국엔 컴퓨터 비젼기술로 찾았을 경우와 , 신경망으로 하였을 경우 두가지로 나뉨

!그림 3- 4

이러한 분석은 $$\phi$$ 에 대해 인사이트를 도출 해주었다. 그림에 해당하는 activation value의 capacity에 대해서 분류가 잘 이루어지지만, 나머지 도메인에서는 작동을 설명하지 않는다. 

###  4. Blind Spots in Neural Networks

지금 까지 유닛의 검사방법은 DNN에 의해 학습된 표현의 복잡성에 관한 직관성을 확인하는 것 외에는 유용성이 없었다. 전역 네트워크 레벨 검사 방법은 모델에 의해 만들어진 분류 결정을 설명하는데 유용할 수 있으며, 주어진 시각적 입력 인스턴스를 올바르게 분류한 입력 부분을 식별하는 데 사용 가능하다.

전역 분석은 훈련 된 네트워크가 나타내는 입력-출력 매핑을 더 잘 이해할 수 있다는 점에서 유용하다.

일반적으로 신경망의 출력 레이어 유닛은 입력에 대한 비선형 결과값이다.





rubost :: 이상치/에러값으로부터 영향을 크게 받지 않는



capacity of $$\phi$$ 









최소한의 필요한 perturbation의 정확한 구성이 서로 다른 역전파

